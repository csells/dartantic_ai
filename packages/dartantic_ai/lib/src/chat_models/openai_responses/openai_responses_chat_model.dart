import 'dart:async';
import 'dart:convert';

import 'package:dartantic_interface/dartantic_interface.dart';
import 'package:http/http.dart' as http;
import 'package:json_schema/json_schema.dart';
import 'package:logging/logging.dart';
import 'package:openai_core/openai_core.dart' as openai;

import '../../agent/tool_constants.dart';
import '../../retry_http_client.dart';
import 'openai_responses_chat_options.dart';
import 'openai_responses_event_mapper.dart';
import 'openai_responses_invocation_builder.dart';
import 'openai_responses_server_side_tool_mapper.dart';

/// Chat model backed by the OpenAI Responses API.
class OpenAIResponsesChatModel
    extends ChatModel<OpenAIResponsesChatModelOptions> {
  /// Creates a new OpenAI Responses chat model instance.
  OpenAIResponsesChatModel({
    required super.name,
    required super.defaultOptions,
    super.tools,
    super.temperature,
    this.baseUrl,
    this.apiKey,
    http.Client? httpClient,
  }) : _client = openai.OpenAIClient(
         apiKey: apiKey,
         // openai_core requires non-nullable baseUrl, use Responses endpoint
         // as default
         baseUrl: baseUrl?.toString() ?? 'https://api.openai.com/v1/responses',
         httpClient: RetryHttpClient(inner: httpClient ?? http.Client()),
       );

  static final Logger _logger = Logger(
    'dartantic.chat.models.openai_responses',
  );

  final openai.OpenAIClient _client;

  /// Base URL override for the OpenAI API.
  final Uri? baseUrl;

  /// API key used for authentication.
  final String? apiKey;

  @override
  List<Tool>? get tools {
    // Filter out return_result from the tools list since we handle
    // outputSchema natively. See wiki/Typed-Output-Architecture.md for the
    // rationale behind delegating the removal to providers with native JSON
    // support.
    final baseTools = super.tools;
    if (baseTools == null) return null;
    return baseTools
        .where((tool) => tool.name != kReturnResultToolName)
        .toList();
  }

  List<openai.Tool> _buildFunctionTools() {
    final registeredTools = tools; // Already filtered by the getter
    if (registeredTools == null || registeredTools.isEmpty) {
      return const [];
    }

    final mapped = registeredTools
        .map(
          (tool) => openai.FunctionTool(
            name: tool.name,
            description: tool.description,
            parameters: Map<String, dynamic>.from(
              tool.inputSchema.schemaMap ?? {},
            ),
          ),
        )
        .toList(growable: false);
    return mapped;
  }

  @override
  Stream<ChatResult<ChatMessage>> sendStream(
    List<ChatMessage> messages, {
    OpenAIResponsesChatModelOptions? options,
    JsonSchema? outputSchema,
  }) async* {
    final invocation = _buildInvocation(messages, options, outputSchema);
    _validateInvocation(invocation);
    final responseStream = await _sendRequest(invocation);
    final mapper = _createMapper(invocation);
    yield* _consumeResponseStream(responseStream, mapper);
  }

  @override
  void dispose() => _client.close();

  /// Downloads a file from a code interpreter container.
  ///
  /// Files are generated by server-side code interpreter tool execution
  /// and need to be retrieved separately after the response completes.
  Future<ContainerFileData> downloadContainerFile(
    String containerId,
    String fileId,
  ) async {
    _logger.fine('Downloading container file: $fileId from $containerId');

    // Get filename from metadata using workaround
    final fileName = await _retrieveContainerFileNameWorkaround(
      containerId,
      fileId,
    );

    final bytes = await _client.retrieveContainerFileContent(
      containerId,
      fileId,
    );

    return ContainerFileData(bytes: bytes, fileName: fileName);
  }

  /// TODO: Remove this workaround once openai_core is fixed.
  ///
  /// TEMPORARY WORKAROUND for https://github.com/meshagent/openai_core/issues/6
  ///
  /// The OpenAI API returns "bytes": null when retrieving container file
  /// metadata, but openai_core 0.10.1's ContainerFile.fromJson() expects bytes
  /// to always be a non-null integer (line 165), causing a type cast error.
  ///
  /// This method manually calls the API and parses only the 'path' field,
  /// avoiding the broken fromJson() method.
  ///
  /// When openai_core is fixed, delete this method and use: final metadata =
  ///   await _client.retrieveContainerFile(containerId, fileId); return
  ///   _extractFileName(metadata.path);
  Future<String> _retrieveContainerFileNameWorkaround(
    String containerId,
    String fileId,
  ) async {
    try {
      final uri = Uri.parse(
        'https://api.openai.com/v1/containers/$containerId/files/$fileId',
      );

      final effectiveApiKey = apiKey ?? _client.apiKey;
      if (effectiveApiKey == null) {
        throw Exception('No API key available for retrieving file metadata');
      }

      _logger.fine('Fetching container file metadata from $uri');

      // Create request manually to ensure proper headers
      final request = http.Request('GET', uri)
        ..headers.addAll({
          'Authorization': 'Bearer $effectiveApiKey',
          'OpenAI-Beta': 'responses=v1',
        });

      final streamedResponse = await _client.httpClient.send(request);
      final response = await http.Response.fromStream(streamedResponse);

      _logger.fine('Container file metadata response: ${response.statusCode}');

      if (response.statusCode != 200) {
        _logger.warning(
          'Failed to retrieve container file metadata: ${response.statusCode} '
          '${response.body}',
        );
        return fileId; // Fallback to fileId
      }

      final json = jsonDecode(response.body) as Map<String, dynamic>;
      final path = json['path'] as String?;

      if (path == null || path.isEmpty) {
        _logger.fine('No path in metadata, using fileId');
        return fileId; // Fallback to fileId
      }

      final extractedName = _extractFileName(path);
      _logger.fine('Extracted filename: $extractedName from path: $path');
      return extractedName;
    } on Exception catch (e, stackTrace) {
      _logger.warning(
        'Error retrieving container file metadata: $e\n$stackTrace',
      );
      return fileId; // Fallback to fileId on any error
    }
  }

  /// Extracts the filename from a container file path.
  ///
  /// Example: '/mnt/data/fibonacci.csv' â†’ 'fibonacci.csv'
  String _extractFileName(String path) {
    final segments = path.split('/');
    final fileName = segments.isNotEmpty ? segments.last : path;
    return fileName.isEmpty ? path : fileName;
  }

  List<openai.Tool> _buildAllTools(OpenAIServerSideToolContext context) => [
    ..._buildFunctionTools(),
    ...OpenAIResponsesServerSideToolMapper.buildServerSideTools(
      serverSideTools: context.enabledTools,
      fileSearchConfig: context.fileSearchConfig,
      webSearchConfig: context.webSearchConfig,
      codeInterpreterConfig: context.codeInterpreterConfig,
      imageGenerationConfig: context.imageGenerationConfig,
    ),
  ];

  OpenAIResponsesInvocation _buildInvocation(
    List<ChatMessage> messages,
    OpenAIResponsesChatModelOptions? options,
    JsonSchema? outputSchema,
  ) => OpenAIResponsesInvocationBuilder(
    messages: messages,
    options: options,
    defaultOptions: defaultOptions,
    outputSchema: outputSchema,
  ).build();

  void _validateInvocation(OpenAIResponsesInvocation invocation) {
    if ((invocation.serverSide.codeInterpreterConfig?.shouldReuseContainer ??
            false) &&
        !invocation.store) {
      _logger.warning(
        'Code interpreter container reuse requested but store=false; '
        'previous_response_id will not be persisted.',
      );
    }
  }

  Future<openai.ResponseStream> _sendRequest(
    OpenAIResponsesInvocation invocation,
  ) async {
    final allTools = _buildAllTools(invocation.serverSide);

    return _client.streamResponse(
      model: openai.ChatModel(name),
      input: invocation.history.input,
      instructions: invocation.history.instructions,
      previousResponseId: invocation.history.previousResponseId,
      store: invocation.store,
      temperature: invocation.parameters.temperature ?? temperature,
      topP: invocation.parameters.topP,
      maxOutputTokens: invocation.parameters.maxOutputTokens,
      reasoning: invocation.parameters.reasoning,
      text: invocation.parameters.textFormat,
      toolChoice: invocation.parameters.toolChoice,
      tools: allTools.isEmpty ? null : allTools,
      parallelToolCalls: invocation.parameters.parallelToolCalls,
      metadata: invocation.parameters.metadata,
      include: invocation.parameters.include,
      truncation: invocation.parameters.truncation,
      user: invocation.parameters.user,
    );
  }

  OpenAIResponsesEventMapper _createMapper(
    OpenAIResponsesInvocation invocation,
  ) => OpenAIResponsesEventMapper(
    storeSession: invocation.store,
    downloadContainerFile: downloadContainerFile,
  );

  Stream<ChatResult<ChatMessage>> _consumeResponseStream(
    openai.ResponseStream responseStream,
    OpenAIResponsesEventMapper mapper,
  ) async* {
    try {
      await for (final event in responseStream.events) {
        _logger.fine('Received event: ${event.runtimeType}');
        yield* mapper.handle(event);
      }
    } on Object catch (error, stackTrace) {
      _logger.severe(
        'OpenAI Responses stream error: $error',
        error,
        stackTrace,
      );
      rethrow;
    }
  }
}
