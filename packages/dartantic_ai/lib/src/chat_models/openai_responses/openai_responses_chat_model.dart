import 'dart:async';

import 'package:dartantic_interface/dartantic_interface.dart';
import 'package:http/http.dart' as http;
import 'package:json_schema/json_schema.dart';
import 'package:logging/logging.dart';
import 'package:openai_core/openai_core.dart' as openai;

import '../../retry_http_client.dart';
import 'openai_responses_chat_options.dart';
import 'openai_responses_event_mapper.dart';
import 'openai_responses_invocation_builder.dart';
import 'openai_responses_server_side_tool_mapper.dart';

/// Chat model backed by the OpenAI Responses API.
class OpenAIResponsesChatModel
    extends ChatModel<OpenAIResponsesChatModelOptions> {
  /// Creates a new OpenAI Responses chat model instance.
  OpenAIResponsesChatModel({
    required super.name,
    required super.defaultOptions,
    super.tools,
    super.temperature,
    this.baseUrl,
    this.apiKey,
    http.Client? httpClient,
  }) : _client = openai.OpenAIClient(
         apiKey: apiKey,
         // openai_core requires non-nullable baseUrl, use Responses endpoint
         // as default
         baseUrl: baseUrl?.toString() ?? 'https://api.openai.com/v1/responses',
         httpClient: RetryHttpClient(inner: httpClient ?? http.Client()),
       );

  static final Logger _logger = Logger(
    'dartantic.chat.models.openai_responses',
  );

  final openai.OpenAIClient _client;

  /// Base URL override for the OpenAI API.
  final Uri? baseUrl;

  /// API key used for authentication.
  final String? apiKey;

  List<openai.Tool> _buildFunctionTools() {
    final registeredTools = tools;
    if (registeredTools == null || registeredTools.isEmpty) {
      return const [];
    }

    final mapped = registeredTools
        .map(
          (tool) => openai.FunctionTool(
            name: tool.name,
            description: tool.description,
            parameters: Map<String, dynamic>.from(
              tool.inputSchema.schemaMap ?? {},
            ),
          ),
        )
        .toList(growable: false);
    return mapped;
  }

  @override
  Stream<ChatResult<ChatMessage>> sendStream(
    List<ChatMessage> messages, {
    OpenAIResponsesChatModelOptions? options,
    JsonSchema? outputSchema,
  }) async* {
    final invocation = _buildInvocation(messages, options, outputSchema);
    _validateInvocation(invocation);
    final responseStream = await _sendRequest(invocation);
    final mapper = _createMapper(invocation);
    yield* _consumeResponseStream(responseStream, mapper);
  }

  @override
  void dispose() => _client.close();

  /// Downloads a file from a code interpreter container.
  ///
  /// Files are generated by server-side code interpreter tool execution
  /// and need to be retrieved separately after the response completes.
  Future<ContainerFileData> downloadContainerFile(
    String containerId,
    String fileId,
  ) async {
    _logger.fine('Downloading container file: $fileId from $containerId');

    final metadata = await _client.retrieveContainerFile(containerId, fileId);
    final fileName = _extractFileName(metadata.path);

    final bytes = await _client.retrieveContainerFileContent(
      containerId,
      fileId,
    );

    return ContainerFileData(bytes: bytes, fileName: fileName);
  }

  /// Extracts the filename from a container file path.
  ///
  /// Example: '/mnt/data/fibonacci.csv' â†’ 'fibonacci.csv'
  String _extractFileName(String path) {
    final segments = path.split('/');
    final fileName = segments.isNotEmpty ? segments.last : path;
    return fileName.isEmpty ? path : fileName;
  }

  List<openai.Tool> _buildAllTools(OpenAIServerSideToolContext context) => [
    ..._buildFunctionTools(),
    ...OpenAIResponsesServerSideToolMapper.buildServerSideTools(
      serverSideTools: context.enabledTools,
      fileSearchConfig: context.fileSearchConfig,
      webSearchConfig: context.webSearchConfig,
      codeInterpreterConfig: context.codeInterpreterConfig,
      imageGenerationConfig: context.imageGenerationConfig,
    ),
  ];

  OpenAIResponsesInvocation _buildInvocation(
    List<ChatMessage> messages,
    OpenAIResponsesChatModelOptions? options,
    JsonSchema? outputSchema,
  ) => OpenAIResponsesInvocationBuilder(
    messages: messages,
    options: options,
    defaultOptions: defaultOptions,
    outputSchema: outputSchema,
  ).build();

  void _validateInvocation(OpenAIResponsesInvocation invocation) {
    if ((invocation.serverSide.codeInterpreterConfig?.shouldReuseContainer ??
            false) &&
        !invocation.store) {
      _logger.warning(
        'Code interpreter container reuse requested but store=false; '
        'previous_response_id will not be persisted.',
      );
    }
  }

  Future<openai.ResponseStream> _sendRequest(
    OpenAIResponsesInvocation invocation,
  ) async {
    final allTools = _buildAllTools(invocation.serverSide);

    return _client.streamResponse(
      model: openai.ChatModel(name),
      input: invocation.history.input,
      instructions: invocation.history.instructions,
      previousResponseId: invocation.history.previousResponseId,
      store: invocation.store,
      temperature: invocation.parameters.temperature ?? temperature,
      topP: invocation.parameters.topP,
      maxOutputTokens: invocation.parameters.maxOutputTokens,
      reasoning: invocation.parameters.reasoning,
      text: invocation.parameters.textFormat,
      toolChoice: invocation.parameters.toolChoice,
      tools: allTools.isEmpty ? null : allTools,
      parallelToolCalls: invocation.parameters.parallelToolCalls,
      metadata: invocation.parameters.metadata,
      include: invocation.parameters.include,
      truncation: invocation.parameters.truncation,
      user: invocation.parameters.user,
    );
  }

  OpenAIResponsesEventMapper _createMapper(
    OpenAIResponsesInvocation invocation,
  ) => OpenAIResponsesEventMapper(
    storeSession: invocation.store,
    downloadContainerFile: downloadContainerFile,
  );

  Stream<ChatResult<ChatMessage>> _consumeResponseStream(
    openai.ResponseStream responseStream,
    OpenAIResponsesEventMapper mapper,
  ) async* {
    try {
      await for (final event in responseStream.events) {
        _logger.fine('Received event: ${event.runtimeType}');
        yield* mapper.handle(event);
      }
    } on Object catch (error, stackTrace) {
      _logger.severe(
        'OpenAI Responses stream error: $error',
        error,
        stackTrace,
      );
      rethrow;
    }
  }
}
