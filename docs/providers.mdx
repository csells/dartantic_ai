---
title: Providers
description: "Providers for chat and embeddings models."
---

Out of the box support for 12 providers, with more to come.

## Provider Capabilities

| Provider | Default Model | Default Embedding Model | Capabilities | Notes |
|----------|---------------|------------------------|--------------|-------|
| **OpenAI** | `gpt-4o` | `text-embedding-3-small` | Chat, Embeddings, Vision, Tools, Streaming | Full feature support |
| **OpenAI Responses** | `gpt-4o` | `text-embedding-3-small` | Chat, Embeddings, Vision, Tools, Streaming, Thinking | Includes built-in server-side tools |
| **Anthropic** | `claude-3-5-sonnet-20241022` | - | Chat, Vision, Tools, Streaming | No embeddings |
| **Google** | `gemini-2.0-flash-exp` | `text-embedding-004` | Chat, Embeddings, Vision, Tools, Streaming | Native Gemini API |
| **Mistral** | `mistral-large-latest` | `mistral-embed` | Chat, Embeddings, Tools, Streaming | European servers |
| **Cohere** | `command-r-plus` | `embed-english-v3.0` | Chat, Embeddings, Tools, Streaming | RAG-optimized |
| **Ollama** | `llama3.2:latest` | - | Chat, Tools, Streaming | Local models only |
| **LlamaCpp** | (path to GGUF file) | - | Chat, Streaming | Local GGUF models, no API needed |
| **OpenRouter** | `openai/gpt-4o` | - | Chat, Vision, Tools, Streaming | Multi-model gateway |
| **Together AI** | `meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo` | - | Chat, Tools, Streaming | Open source models |
| **Google-OpenAI** | `gemini-2.0-flash-exp` | - | Chat, Vision, Tools, Streaming | Gemini via OpenAI API |
| **Ollama-OpenAI** | `llama3.2:latest` | - | Chat, Tools, Streaming | Ollama via OpenAI API |

## Provider Configuration

| Provider | Provider Prefix | Aliases | API Key | Provider Type |
|----------|----------------|---------|---------|---------------|
| **OpenAI** | `openai` | - | `OPENAI_API_KEY` | `OpenAIProvider` |
| **OpenAI Responses** | `openai-responses` | - | `OPENAI_API_KEY` | `OpenAIResponsesProvider` |
| **Anthropic** | `anthropic` | `claude` | `ANTHROPIC_API_KEY` | `AnthropicProvider` |
| **Google** | `google` | `gemini`, `googleai` | `GEMINI_API_KEY` | `GoogleProvider` |
| **Mistral** | `mistral` | - | `MISTRAL_API_KEY` | `MistralProvider` |
| **Cohere** | `cohere` | - | `COHERE_API_KEY` | `CohereProvider` |
| **Ollama** | `ollama` | - | None (local) | `OllamaProvider` |
| **LlamaCpp** | `llama_cpp` | - | None (local) | `LlamaCppProvider` |
| **OpenRouter** | `openrouter` | - | `OPENROUTER_API_KEY` | `OpenAIProvider` |
| **Together AI** | `togetherai` | `together` | `TOGETHER_API_KEY` | `OpenAIProvider` |
| **Google-OpenAI** | `google-openai` | - | `GEMINI_API_KEY` | `OpenAIProvider` |
| **Ollama-OpenAI** | `ollama-openai` | - | None (local) | `OpenAIProvider` |

## Setup

```bash
# Set API keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="..."
```

## Usage

```dart
// Basic
Agent('openai');

// With model
Agent('anthropic:claude-3-5-sonnet');

// Chat + embeddings
Agent('openai?chat=gpt-4o&embeddings=text-embedding-3-large');

// Responses API features (thinking, server-side tools)
Agent('openai-responses:gpt-5');

// Local GGUF model with LlamaCpp (use query parameter for file paths)
Agent('llama_cpp?chat=/path/to/your/model.gguf');
```

## Find Providers

```dart
// All providers
Providers.all

// By capability
Providers.allWith({ProviderCaps.chatVision})

// By name
Providers.get('claude') // â†’ anthropic
```

## Custom Config

```dart
final provider = OpenAIProvider(
  apiKey: 'key',
  baseUrl: Uri.parse('https://custom.api.com/v1'),
);
Agent.forProvider(provider);
```

## Check Capabilities

```dart
// Check what a provider supports
final agent = Agent('openrouter');
print('Capabilities: ${provider.caps}');
// Output: {chat, vision, toolCalls, streaming}

// Check specific capability
if (provider.caps.contains(ProviderCaps.embeddings)) {
  final embed = await agent.embedQuery('test');
} else {
  print('Provider does not support embeddings');
}
```

### Available Capabilities

- **`chat`** - Chat conversations
- **`embeddings`** - Vector embeddings
- **`chatVision`** - Image/file processing handled by the chat model
- **`toolCalls`** - Function calling
- **`multiToolCalls`** - Multiple tool calls
- **`streaming`** - Stream responses
- **`typedOutput`** - Structured output
- **`thinking`** - Provider streams reasoning metadata (e.g. OpenAI Responses)

## List Models

```dart
// List all models from a provider
final provider = Providers.openai;
await for (final model in provider.listModels()) {
  final status = model.stable ? 'stable' : 'preview';
  print('${model.name}: ${model.displayName} [$status]');
}

// Example output:
// - openai:gpt-4-0613  (chat)
// - openai:gpt-4  (chat)
// - openai:gpt-3.5-turbo  (chat)
```

## Local Models with LlamaCpp

LlamaCpp enables running GGUF format models locally without any API calls or internet connection.

### Requirements

1. A GGUF model file (download from [Hugging Face](https://huggingface.co/models?library=gguf))
2. The llama.cpp shared library (automatically included with `llama_cpp_dart` package)

### Usage

```dart
// Using Agent API with model path (use query parameter format)
final agent = Agent('llama_cpp?chat=/path/to/model.gguf');
final result = await agent.send('What is AI?');
print(result.output);

// Streaming support
await for (final chunk in agent.sendStream('Explain quantum physics')) {
  stdout.write(chunk.output);
}

// Using Provider API for advanced configuration
final provider = LlamaCppProvider(
  modelPath: '/path/to/model.gguf',
  promptFormat: ChatMLFormat(), // or AlpacaFormat(), GeminiFormat(), etc.
  contextParams: ContextParams(nCtx: 4096),
  samplerParams: SamplerParams(
    temperature: 0.7,
    topP: 0.9,
  ),
);
final model = provider.createChatModel(temperature: 0.8);
```

### Prompt Formats

LlamaCpp supports multiple prompt formats:
- `ChatMLFormat()` - ChatML format (default)
- `AlpacaFormat()` - Alpaca format
- `GeminiFormat()` - Gemini format

### Limitations

- **No tool calling**: LlamaCpp doesn't support function calling
- **No typed output**: JSON schema validation not supported
- **No embeddings**: Only chat models supported
- **No model listing**: Uses local files only

### Example Models

Download GGUF models from Hugging Face:
- [Llama 2 7B Chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)
- [Mistral 7B Instruct](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
- [Phi-3 Mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf)

## Examples

- [LlamaCpp local models](https://github.com/csells/dartantic_ai/blob/main/packages/dartantic_ai/example/bin/llama_cpp_example.dart)
- [List all provider models](https://github.com/csells/dartantic_ai/blob/main/packages/dartantic_ai/example/bin/provider_models.dart)
- [Multi-provider conversations](https://github.com/csells/dartantic_ai/blob/main/packages/dartantic_ai/example/bin/multi_provider.dart)
- [Custom providers](https://github.com/csells/dartantic_ai/blob/main/packages/dartantic_ai/example/bin/custom_provider.dart)
